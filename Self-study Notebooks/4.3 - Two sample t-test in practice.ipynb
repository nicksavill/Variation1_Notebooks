{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two sample *t*-test in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**In this notebook you will learn how to perform a two-sample *t*-test in practice using Python and how to report the outcome of the test.**\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard statistical test for comparing two population means is called the **two sample *t*-test**. The \"two sample\" part is obvious: we have two samples one from 1976 and another from 1978. The \"*t*-test\" part means we use the *t*-statistic to test the null hypothesis. \n",
    "\n",
    "The *t*-statistic is similar to the *d*-statistic we used in the last notebook. Recall that the *d*-statistic is the absolute difference in the sample means, i.e., \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "$$d = |\\bar{x}_\\text{1978} - \\bar{x}_\\text{1976}|$$\n",
    "\n",
    "</div>\n",
    "\n",
    "The problem with the *d*-statistic is that we have to simulate its sampling distribution to calculate a *p*-value. We really don't want to do that every time we want to compare two population means. \n",
    "\n",
    "Fortunately there is a solution to this problem and that is to use something called the *t*-statistic.\n",
    "\n",
    "The *t*-statistic is similar to the *d*-statistic in that it measures the difference between the two sample means. But the difference in sample means is divided by something called the standard error of the difference in the sample means. The good news is that we don't need to concern ourselves with how the *t*-statistic is calculated. All we need to know is that it measures how far apart the two sample means are. The farther apart they are the larger the *t*-statistic will be. \n",
    "\n",
    "Large values of the *d*-statistic and the *t*-statistic are unlikely if the null hypothesis were true (the *p*-value will be small). Small values of the *d*-statistic and the *t*-statistic are likely if the null hypothesis were true  (the *p*-value will be large).\n",
    "\n",
    "The great thing about the *t*-statistic is that you don't need to simulate a statistical model to obtain its sampling distribution. The purpose of making you simulate thousands of samples in the last notebook was to help you understand how test statistics are calculated, how they are used to obtain a *p*-value and how to interpret a *p*-value.\n",
    "\n",
    "Now that you know what a test statistic is, what its sampling distribution is and what a *p*-value means we can now use the standard statistical test for comparing two population means.\n",
    "\n",
    "To perform a two sample *t*-test in Python we need to import the code that performs the test from the `scipy.stats` module.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "1. Read the code so that you understand what it does.\n",
    "2. Run the code cell to perform the t-test.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind # import the two sample t-test function\n",
    "\n",
    "beak_depth = pd.read_csv('Datasets/finches beak depth.csv')\n",
    "\n",
    "\n",
    "# Perform the two sample t-test\n",
    "# Include nan_policy='omit' because there are missing values in the 1978 data that need to be ignored. Without this the test won't work.\n",
    "t, p = ttest_ind( beak_depth['1978'], beak_depth['1976'], nan_policy='omit' )\n",
    "\n",
    "\n",
    "# Print the t-statistic to 2dp\n",
    "print(f't-statistic = {t:.2f}')\n",
    "\n",
    "# Print the p-value to 4dp\n",
    "print(f'p-value = {p:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two sample *t*-test is performed by the function `ttest_ind()`. The beak depths of the two samples are passed to the function and it returns the *t*-statistic and the *p*-value. All of the hard calculations and complicated maths are done for you by the computer. All we care about is the *p*-value.\n",
    "\n",
    "It's as simple as that.\n",
    "\n",
    "The value of the *t*-statistic is 3.18. Remember the *t*-statistic measures the difference between the sample means; the larger the value of the *t*-statistic the further apart the samples means are, and the less likely the value of the *t*-statistic is if the null hypothesis were true. \n",
    "\n",
    "The result is saying that the probability of the value of the *t*-statistic being at least 3.18 is 0.0019 (about 1 in 500) if the null hypothesis were true, i.e., very unlikely. Which means it is very unlikely that the population mean beak depths of the 1976 and 1978 finches are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To reject or not reject the null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that null hypothesis significance testing is designed to challenge or “reject” the null hypothesis. We are not testing the alternative hypothesis.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "If a test statistic is inconsistent with the prediction of the null hypothesis (i.e., when the *p*-value is small) then we **reject the null hypothesis**. \n",
    "    \n",
    "</div>\n",
    "\n",
    "This begs the question of how inconsistent does the test statistic have to be? There is no right or wrong answer to that question. Which means we have to rely on convention. In biology and medicine the convention is that when the *p*-value is below 0.05 (1 in 20) then the test statistic is considered inconsistent with the null hypothesis. And this means we reject the null hypothesis. \n",
    "\n",
    "This value of 0.05 is called **the significance level** and is usually denoted by the greek letter $\\alpha$ (alpha). So in biology and medicine the conventional significance level of null hypothesis significance testing is $\\alpha$ = 0.05. \n",
    "\n",
    "The significance level is the probability of rejecting the null hypothesis when it is actually true. For example, a significance level of 0.05 means you have a 1 in 20 chance of concluding that a difference exists when, in fact, there is no actual difference.\n",
    "\n",
    "We have calculated a *p*-value of 0.0019 which is below $\\alpha$ = 0.05. So, by convention, we reject the null hypothesis. But there is a 1 in 20 chance we could be wrong. As we have seen, a difference of 0.49 mm in beak depths is not impossible if the null hypothesis were true, it's just very unlikely.\n",
    "\n",
    "Note that we cannot prove that the alternative hypothesis is true. The best we can do is show that the data are unlikely under the null hypothesis, and, therefore, likely under the alternative hypothesis.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "If a test statistic is consistent with the prediction of the null hypothesis then we **fail to reject the null hypothesis**. \n",
    "\n",
    "</div>\n",
    "\n",
    "This happens when the *p*-value is greater than $\\alpha$ = 0.05.\n",
    "\n",
    "Failing to reject the null hypothesis doesn't mean we accept it. Accepting the null hypothesis implies that we accept that no difference exists. But there could be one of several reasons why we didn't find a difference:\n",
    "1. The sample size was too small to detect a difference.\n",
    "2. The variability in the data was too high. The difference exists, but the variability in the data swamped the difference.\n",
    "3. When dealing with random samples, chance always plays a role in the results. The luck of the draw might have caused the samples not to reflect a real difference that exists between populations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting the result of the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after collecting beak depth data, performing a *t*-test and rejecting the null hypothesis the researchers need to publish and report their results. This is how they might report it:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "\"Beak depths of medium ground finches were measured on the Galápagos island of Daphne Major in 1976 and 1978.    A statistically significant difference was found of 0.49 mm between beak depths in the two years (*t* = 3.18, *p* = 0.0019). This evidence supports the hypothesis that the 1977 drought caused selection for larger beaks on Daphne Major.\"\n",
    "</div>\n",
    "\n",
    "When a null hypothesis is rejected then the result is said to be **statistically significant** or just **significant**. Do not read too much in to the word \"significant\". Significant does not mean \"important\" as it does in English. In statistics it means that an observed difference between two or more populations is likely.\n",
    "\n",
    "When a null hypothesis is not rejected then the result is said to be **not statistically significant** or **non-significant**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common misinterpretations of the *p*-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last few notebooks have tried to give you an intuitive understanding of the *p*-value. \n",
    "\n",
    "The *p*-value is the probability of getting a test statistic that is at least as unusual as the one you observed in your experiment or observational study if the null hypothesis were true. \n",
    "\n",
    "Once you've calculated your *p*-value from your data and chosen a significance level (conventionally $\\alpha$ = 0.05) there are only **two** things your are allowed to say:\n",
    "1. If *p* < 0.05, you found a statistically significant result and you reject the null hypothesis, or\n",
    "2.  if *p* > 0.05, you found a non-statistically significant result and you fail to reject the null hypothesis.\n",
    "\n",
    "That's it! You cannot make any more claims about your data and results than that. \n",
    "\n",
    "Say you found *p* = 0.06 (6%), a result close to significant. Although you maybe tempted to, you **cannot say any of the following**:\n",
    "1. The probability the null model is true is 6%. \n",
    "\n",
    "2. The probability the alternative model is false is 6%.\n",
    "\n",
    "    - The probabilities of the null and alternative models are either 0% or 100% (either false or true) and you will never know which. But what you have is evidence (in the form of data) to reject or not reject the null hypothesis.\n",
    "\n",
    "3. The collection of more data will make the result significant (i.e., *p* will get smaller).\n",
    "    \n",
    "    - There's actually about a 20% chance (1 in 5) of getting a less significant result (*p* gets higher) if you double the amount of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Two sample t-test in practice](Exercises/4.3%20-%20Two%20sample%20t-test%20in%20practice.ipynb)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b34568b434bb6e5bf895c3fbf2a1e101e1e281d0c9e435769d397db7902ae33"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
